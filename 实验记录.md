# 目录

[TOC]



# 实验记录 2019-11-08

##可以读dblp的数据

之前尝试使用spark-xml读取dblp的数据，但是读取过程中发生OOM异常。

一次性读取全部dblp数据的代码如下：

```scala
    val df = spark.read
      .format("com.databricks.spark.xml")
      .option("rowTag", "dblp")
      .load("file:///root/dblp.xml")
```

这种读取的方式导致spark的Excutor端生成了过于庞大的SchemaRDD，导致了OOM异常。在查阅了spark-xml的README.md文件后，最终发现了正确的读取方式。



README.md中option支持的选项部分找到关于`rowTag`和`rootTag`的说明：

>* `rowTag`: The row tag of your xml files to treat as a row. For example, in this xml `<books> <book><book> ...</books>`, the appropriate value would be `book`. Default is `ROW`.
>* `rootTag`: The root tag of your xml files to treat as the root. For example, in this xml `<books> <book><book> ...</books>`, the appropriate value would be `books`. Default is `ROWS`.

简单的说`rowTag`选项是用来设置要将什么xml节点转换为DataFrame的row，而`rootTag`用来设置要将根节点设置为哪个节点。

这样就解释了为什么一次性读取dblp的代码会导致OOM异常。因为dblp.xml的根节点是dblp节点，但是却被当成了row。这会在Excutor端生成了一个巨大的RDD对象，导致OOM。



能够不导致OOM的参数设置如下：

```scala
    val df = spark.read
      .format("com.databricks.spark.xml")
      .option("rootTag", "dblp")
 			.option("rowTag", "article")
      .load("file:///root/dblp.xml")
```

避免一次读取全部dblp.xml数据，采用分别读取dblp根节点下的子节点的方式读取。



但是dblp数据集的大小有2g，我们没法知道dblp下一层的子节点都是什么？于是参考了dblp.dtd文件，得知了dblp下的字节点名称。并对各个字节点的记录数量进行了统计。

| subElement    | Count   |
| ------------- | ------- |
| article       | 2093906 |
| inproceedings | 2450274 |
| proceedings   | 41782   |
| book          | 17714   |
| incollection  | 59477   |
| phdthesis     | 73252   |
| mastersthesis | 12      |
| www           | 2346866 |
| person        | 0       |
| data          | 0       |

下一步准备将各个子节点写入单独的xml文件里

#实验记录 2019-11-14

##修改了dblp中的实体定义为latin-1编码

读取文件时发现出现了大量的记录显示corrupt_record

| _corrupt_record | _key | _mdate | _publtype | author | ee   |
| --------------- | ---- | ------ | --------- | ------ | ---- |
|                null|     tr/meltdown/s18|2018-01-07| informal|[Paul Kocher, Dan...|                null|https://spectreat...|
|<article mdate="2...|                null|      null|     null|                null|                null|                null|
|<article mdate="2...|                null|      null|     null|                null|                null|                null|


排查以后发现是由于dblp.xml存在一些xml实体定义导致spark-xml不能正常读取。

```xml
    <article mdate="2017-06-08" key="tr/ibm/IWBS191" publtype="informal">
        <author>Rolf Sander</author>
        <title>Die Repr&auml;sentation r&auml;umlichen Wissens und die Behandlung von Einbettungsproblemen mit Quadtreedepiktionen
        </title>
        <journal>IWBS Report</journal>
        <volume>191</volume>
        <year>1991</year>
        <publisher>IBM Germany Science Center, Institute for Knowledge Based Systems</publisher>
    </article>
```

查阅了spark-xml的readme没有发现可以根据dtd的实体定义替换为对应的latin-1编码字符的方法，于是打算将实体定义相关的字符串用正则表达式匹配并替换。读取dblp.xml替换相应的字符串以后写到dblp_after.xml文件中。

```scala
import java.io.File
import org.apache.spark.sql.SparkSession
object Test4 {
  val s1 = "file:////Users/gaoxin/WorkSpace/spark/dblp.xml"
  //将实体替换以后的dblp写在这里
  val s2 = "file:////Users/gaoxin/WorkSpace/spark/dblp_after.xml"

  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("XML_Test")
      .master("local[*]")
      .getOrCreate()

    val subNode = Array("article")
    
    //文本方式读取
    val file = spark.sparkContext.textFile(s1)

    //转换实体
    val afterParse = file.map(s => ReplaceEntity.parse(s))
    
    //afterParse.foreach(it => println(it))

    val afterParedblp: File = new File(s2)

   if (afterParedblp.isFile) {
     println("delete "+ afterParedblp.delete())
   }

    afterParse.saveAsTextFile(s2)
  }
}
```



**将实体名称替换为实体编号的方式尽管可以在ide下正常显示，但是使用控制台的时候仍然显示乱码。应该替换为具体字符。下面的代码并没有根本地解决问题，以后需要改。**

```scala
import scala.util.matching.Regex

object ReplaceEntity {
  val regex: Regex = new Regex("&[A-Za-z]*;")
  def parse(s: String): String = {
    val optS = regex.replaceAllIn(s, it => it.toString() match {
      case "&reg;" => "&#174;"
      case "&micro;" => "&#181;"
      case "&times;" => "&#215;"
      case "&Agrave;" => "&#192;"
      case "&Aacute;" => "&#193;"
      case "&Acirc;" => "&#194;"
//.....省略
      case _ => {
        println(it)
        it.toString()
      }
    })

    optS
  }
}
```

经过实体替换以后的dblp_after.xml可以正常读取，检查了dataframe的记录也没有发现corrupt_record。

#实验记录 2019-11-21

可以进行简单的查询了

##选取特定的列

```scala
    import spark.implicits._
    dblpArticle.select($"title", $"author", $"url").show()
```

| title                | author               | url                  |
| -------------------- | -------------------- | -------------------- |
| Spectre Attacks: ... | [[Paul Kocher,,],... | null                 |
| Meltdown             | [[Moritz Lipp,,],... | null                 |
| An Evaluation of ... | [[Frank Manola,,]]   | db/labs/gte/index... |

## 根据long类型执行过滤

```scala
import spark.implicits._
dblpArticle.filter($"year" > 2000).show()
```

| title                | author               | year                 |
| -------------------- | -------------------- | -------------------- |
|Spectre Attacks: ...|[[Paul Kocher,,],...|2018|
|            Meltdown|[[Moritz Lipp,,],...|2018|

## 字符串正则匹配

```scala
import spark.implicits._
dblpArticle.select($"title", $"author", $"url").filter($"title" rlike "^Knowledge").show()
```
| title                | author               | year                 |
| -------------------- | -------------------- | -------------------- |
|Knowledge in Oper...|[[Toni Bollinger,...|null|

其他的写在测试代码里了

# 实验记录 2020-02-20

通过docker和docker-compose搭建了实验环境。

1. Mongodb环境
2. Spark和HDFS的环境（可在单机进行伪分布式实验）
3. Hadoop Yarn环境（可在单机进行伪分布式实验）

Spark环境和Hadoop环境是以别人的dockerfile为基础改的这是Github的地址：https://github.com/big-data-europe/docker-hadoop

# 实验记录 2020-02-27

lmh将进行过预处理的数据写入了Mongodb，数据可以正常写入，并通过Mongodb读取。

让xxk学习通过SpringBoot-jpa组件读取Mongodb。

# 实验记录 2020-03-10

将之前的研究成果进行整理

建立远程仓库：https://github.com/FHamster/SparkExp

# 实验记录 2020-03-11

## 发现了title字段空白的错误

翻看mongodb内的数据时发现了严重的错误，大量的title字段一片空白。

排查以后发现是spark-xml推导出了不期望的schema，导致title节点的文本全部没读。

spark-xml推测的schema：

```
root
 |-- _cdate: string (nullable = true)
 |-- _key: string (nullable = true)
 |-- _mdate: string (nullable = true)
 |-- _publtype: string (nullable = true)
 |-- author: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- _VALUE: string (nullable = true)
 |    |    |-- _aux: string (nullable = true)
 |    |    |-- _orcid: string (nullable = true)
 |-- booktitle: string (nullable = true)
 |-- cdrom: string (nullable = true)
 |-- cite: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- _VALUE: string (nullable = true)
 |    |    |-- _label: string (nullable = true)
 |-- crossref: string (nullable = true)
 |-- editor: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- _VALUE: string (nullable = true)
 |    |    |-- _orcid: string (nullable = true)
 |-- ee: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- _VALUE: string (nullable = true)
 |    |    |-- _type: string (nullable = true)
 |-- journal: string (nullable = true)
 |-- month: string (nullable = true)
 |-- note: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- _VALUE: string (nullable = true)
 |    |    |-- _type: string (nullable = true)
 |-- number: string (nullable = true)
 |-- pages: string (nullable = true)
 |-- publisher: string (nullable = true)
 |-- title: struct (nullable = true)
 |    |-- _VALUE: string (nullable = true)
 |    |-- _bibtex: string (nullable = true)
 |    |-- i: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- sub: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- sup: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |-- url: string (nullable = true)
 |-- volume: string (nullable = true)
 |-- year: long (nullable = true)

```

定位了导致这种现象的xml记录

```xml
<title>&#120001;<sub>0</sub> Regularized Structured Sparsity Convolutional Neural Networks.</title>

<title>Beyond Ohba's Conjecture: A bound on the choice number of <i>k</i>-chromatic graphs with <i>n</i> vertices.</title>

<title>Asynchronous machine vector control: PI<sup>&#945;</sup> controllers for current loops.</title>
```



在dblp网站的显示效果

* **𝓁**0 Regularized Structured Sparsity Convolutional Neural Networks.
* Beyond Ohba's Conjecture: A bound on the choice number of *k*-chromatic graphs with *n* vertices.
* Average dwell time approach to *H*∞ filter for continuous-time switched linear parameter varying systems with time-varying delay.

**发现文章的标题有特殊的文本效果，但是spark-xml在处理时丢失了这部分的语意。需要进行进一步的预处理。并且这意味**

1. 着其他的dblp下的一级字节点也会有这个问题，需要人工设定不少的schema （头疼）
2. 网页显示工作需要加入这种显示功能 （头更疼）







# 实验记录 2020-04-08

## 尝试解决title空白问题，未成功

尝试手动设定schema以后仍然产生这样的现象，**效果不好**

```json
[
  {
    "title": " Transferability of Adversarial Examples to Attack Cloud-based Image Classifier Service."
  },
  {
    "title": "! and ? - Storage as Tensorial Strength."
  },
  {
    "title": "!MDP Playground: Meta-Features in Reinforcement Learning."
  },
  {
    "title": "\""
  },
  {
    "title": "\""
  },
  {
    "title": "\""
  },
  //这里省略
  [
  {
    "title": "\uD83D\uDC4D as social support: Relational closeness, automaticity, and interpreting social support from paralinguistic digital affordances in social media."
  },
  {
    "title": "\uD835\uDD43"
  },
  {
    "title": "\uD835\uDD3D"
  },
  {
    "title": "\uD835\uDD3D"
  },
  {
    "title": "\uD835\uDCDE(k)-robust spanners in one dimension."
  },
  {
    "title": "\uD835\uDCC1"
  },
  {
    "title": "\uD835\uDCC1"
  },
  {
    "title": "\uD835\uDCC1"
  },
  {
    "title": "\uD835\uDCC1"
  },
  {
    "title": "\uD835\uDCC1"
  },
  {
    "title": "\uD835\uDCB1\uD835\uDCB0-smoothness and proximal point results for some nonconvex functions."
  },
  {
    "title": "\uD835\uDCAB-schemes and Deterministic Polynomial Factoring over Finite Fields."
  },
  {
    "title": "픽"
  },
  {
    "title": "풟-stability performance analysis and stabilization of Sun-Earth "
  },
  {
    "title": "풞-Consistency in signed total graphs of commutative rings."
  }
]
]
```

目前计划通过正则表达式替换```<sub>``` ```<sup>``` ```<i>```这样的标签，在成功读取为dataframe以后再通过dataframe中进行字符串替换

## 对title字段的一些发现

```
 |-- title: struct (nullable = true)
 |    |-- _VALUE: string (nullable = true)
 |    |-- _bibtex: string (nullable = true)
 |    |-- i: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- sub: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- sup: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
```

```_VALUE```:title的文本

```i```:斜体
```sup```:上标
```sub```:下标
```_bibtex```:使用bibtex语法描述的title

